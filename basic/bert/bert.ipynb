{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第9章: 事前学習済み言語モデル（BERT型）\n",
    "本章では、BERT型の事前学習済みモデルを利用して、マスク単語の予測や文ベクトルの計算、評判分析器（ポジネガ分類器）の構築に取り組む。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 80. トークン化\n",
    "“The movie was full of incomprehensibilities.”という文をトークンに分解し、トークン列を表示せよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'movie', 'was', 'full', 'of', 'inc', '##omp', '##re', '##hen', '##si', '##bilities', '.']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "text = \"The movie was full of incomprehensibilities.\"\n",
    "\n",
    "# Tokenize\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 81. マスクの予測\n",
    "“The movie was full of [MASK].”の”[MASK]”を埋めるのに最も適切なトークンを求めよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted token: fun\n",
      "Logit value: 9.2889\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForMaskedLM\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')    # Classifier to predict token for [MASK]\n",
    "model.eval()\n",
    "\n",
    "text = \"The movie was full of [MASK].\"\n",
    "\n",
    "inputs = tokenizer(text, return_tensors='pt')\n",
    "# {\n",
    "#   'input_ids': tensor([[ 101, 1996, 3185, 2001, 2440, 1997,  103, 1012,  102]]), \n",
    "#   'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]]), \n",
    "#   'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
    "# }\n",
    "\n",
    "# Get the index of `[MASK]` in the text\n",
    "mask_token_idx = torch.where(inputs['input_ids'] == tokenizer.mask_token_id)[1] # (row_indices, col_indices)[1]\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits # shape: (batch_size, seq_len, vocab_size)\n",
    "\n",
    "mask_logits = logits[0, mask_token_idx, :]  # shape: (1, 30522) (batch_size, vocab_size)\n",
    "\n",
    "max_logit, max_idx = torch.max(mask_logits, dim=1)\n",
    "\n",
    "best_token_idx  = max_idx.item()\n",
    "best_token = tokenizer.decode([best_token_idx])\n",
    "\n",
    "print(f\"Predicted token: {best_token}\")\n",
    "print(f\"Logit value: {max_logit.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 82. マスクのtop-k予測\n",
    "“The movie was full of [MASK].”の”[MASK]”に埋めるのに適切なトークン上位10個と、その確率（尤度）を求めよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         fun    10.712%\n",
      "   surprises    6.634%\n",
      "       drama    4.468%\n",
      "       stars    2.722%\n",
      "      laughs    2.541%\n",
      "      action    1.952%\n",
      "  excitement    1.904%\n",
      "      people    1.829%\n",
      "     tension    1.503%\n",
      "       music    1.465%\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "probs = F.softmax(mask_logits, dim=1)\n",
    "\n",
    "# Top-10\n",
    "topk = torch.topk(probs, k=10)\n",
    "topk_indices = topk.indices[0].tolist() # indices: (batch_size, vocab_size)\n",
    "topk_probs = topk.values[0].tolist()    # values: (batch_size, vocab_size)\n",
    "\n",
    "for idx, prob  in zip(topk_indices, topk_probs):\n",
    "    token = tokenizer.decode([idx])\n",
    "    print(f\"{token:>12}    {prob * 100:.3f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 83. CLSトークンによる文ベクトル\n",
    "以下の文の全ての組み合わせに対して、最終層の[CLS]トークンの埋め込みベクトルを用いてコサイン類似度を求めよ。\n",
    "\n",
    "“The movie was full of fun.”\n",
    "\n",
    "“The movie was full of excitement.”\n",
    "\n",
    "“The movie was full of crap.”\n",
    "\n",
    "“The movie was full of rubbish.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"The movie was full of fun.\"  ←→  \"The movie was full of excitement.\"   :  cosine-sim = 0.9881\n",
      "\"The movie was full of fun.\"  ←→  \"The movie was full of crap.\"   :  cosine-sim = 0.9558\n",
      "\"The movie was full of fun.\"  ←→  \"The movie was full of rubbish.\"   :  cosine-sim = 0.9475\n",
      "\"The movie was full of excitement.\"  ←→  \"The movie was full of crap.\"   :  cosine-sim = 0.9541\n",
      "\"The movie was full of excitement.\"  ←→  \"The movie was full of rubbish.\"   :  cosine-sim = 0.9487\n",
      "\"The movie was full of crap.\"  ←→  \"The movie was full of rubbish.\"   :  cosine-sim = 0.9807\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# 1. Prepare model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "model.eval()\n",
    "\n",
    "# 2. Prepare sentences\n",
    "texts = [\n",
    "    \"The movie was full of fun.\",\n",
    "    \"The movie was full of excitement.\",\n",
    "    \"The movie was full of crap.\",\n",
    "    \"The movie was full of rubbish.\"\n",
    "]\n",
    "len_texts = len(texts)\n",
    "\n",
    "# 3. \n",
    "cls_embeddings = []\n",
    "with torch.no_grad():\n",
    "    for text in texts:\n",
    "        # Encode\n",
    "        inputs = tokenizer(text, return_tensors='pt')\n",
    "        # Feed the model with inputs\n",
    "        outputs = model(**inputs)\n",
    "        # last_hidden_state: (batch_size, seq_len, hidden_size) (1, seq_len, 768)\n",
    "        cls_vec = outputs.last_hidden_state[:, 0, :]    # shape: (batch_size, hidden_size)   (1, 768)\n",
    "        cls_embeddings.append(cls_vec)\n",
    "\n",
    "cls_embeddings = torch.vstack(cls_embeddings)   # shape: (4, 768)\n",
    "\n",
    "# 4. Compute the cosine similarities for all combinations\n",
    "for i in range(len_texts):\n",
    "    for j in range(i + 1, len_texts):\n",
    "        v1 = cls_embeddings[i]\n",
    "        v2 = cls_embeddings[j]\n",
    "        sim = F.cosine_similarity(v1, v2, dim=0).item()\n",
    "        print(f\"\\\"{texts[i]}\\\"  ←→  \\\"{texts[j]}\\\"   :  cosine-sim = {sim:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 84. 平均による文ベクトル\n",
    "以下の文の全ての組み合わせに対して、最終層の埋め込みベクトルの平均を用いてコサイン類似度を求めよ。\n",
    "\n",
    "“The movie was full of fun.”\n",
    "\n",
    "“The movie was full of excitement.”\n",
    "\n",
    "“The movie was full of crap.”\n",
    "\n",
    "“The movie was full of rubbish.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"The movie was full of fun.\"  ←→  \"The movie was full of excitement.\"   :  cosine-sim = 0.9568\n",
      "\"The movie was full of fun.\"  ←→  \"The movie was full of crap.\"   :  cosine-sim = 0.8490\n",
      "\"The movie was full of fun.\"  ←→  \"The movie was full of rubbish.\"   :  cosine-sim = 0.8169\n",
      "\"The movie was full of excitement.\"  ←→  \"The movie was full of crap.\"   :  cosine-sim = 0.8352\n",
      "\"The movie was full of excitement.\"  ←→  \"The movie was full of rubbish.\"   :  cosine-sim = 0.7938\n",
      "\"The movie was full of crap.\"  ←→  \"The movie was full of rubbish.\"   :  cosine-sim = 0.9226\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# 1. Prepare model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "model.eval()\n",
    "\n",
    "# 2. Prepare sentences\n",
    "texts = [\n",
    "    \"The movie was full of fun.\",\n",
    "    \"The movie was full of excitement.\",\n",
    "    \"The movie was full of crap.\",\n",
    "    \"The movie was full of rubbish.\"\n",
    "]\n",
    "len_texts = len(texts)\n",
    "\n",
    "# 3. \n",
    "avg_embeddings = []\n",
    "with torch.no_grad():\n",
    "    for text in texts:\n",
    "        # Encode\n",
    "        inputs = tokenizer(text, return_tensors='pt')\n",
    "        # Feed the model with inputs\n",
    "        outputs = model(**inputs)\n",
    "        # last_hidden_state: (batch_size, seq_len, hidden_size) (1, seq_len, 768)\n",
    "        avg_vec = torch.mean(outputs.last_hidden_state, dim=1)    # shape: (batch_size, hidden_size)   (1, 768)\n",
    "        avg_embeddings.append(avg_vec)\n",
    "\n",
    "avg_embeddings = torch.vstack(avg_embeddings)   # shape: (4, 768)\n",
    "\n",
    "# 4. Compute the cosine similarities for all combinations\n",
    "for i in range(len_texts):\n",
    "    for j in range(i + 1, len_texts):\n",
    "        v1 = avg_embeddings[i]\n",
    "        v2 = avg_embeddings[j]\n",
    "        sim = F.cosine_similarity(v1, v2, dim=0).item()\n",
    "        print(f\"\\\"{texts[i]}\\\"  ←→  \\\"{texts[j]}\\\"   :  cosine-sim = {sim:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 85. データセットの準備\n",
    "General Language Understanding Evaluation (GLUE) ベンチマークで配布されているStanford Sentiment Treebank (SST) から訓練セット（train.tsv）と開発セット（dev.tsv）のテキストと極性ラベルと読み込み、さらに全てのテキストはトークン列に変換せよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, DataCollatorWithPadding\n",
    "\n",
    "class SSTDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.texts = df['sentence'].tolist()\n",
    "        self.labels = [int(x) for x in df['label'].tolist()]\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encodings = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            return_attention_mask=True,\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(encodings[\"input_ids\"]),\n",
    "            \"attention_mask\": torch.tensor(encodings[\"attention_mask\"]),    # All elements are 1 because padding is not done\n",
    "            \"labels\": torch.tensor(label)\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "train_df = pd.read_csv(\"train.tsv\", sep=\"\\t\")\n",
    "valid_df = pd.read_csv(\"dev.tsv\",   sep=\"\\t\")\n",
    "\n",
    "train_dataset = SSTDataset(train_df, tokenizer)\n",
    "valid_dataset   = SSTDataset(valid_df, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 86. ミニバッチの作成\n",
    "85で読み込んだ訓練データの一部（例えば冒頭の4事例）に対して、パディングなどの処理を行い、トークン列の長さを揃えてミニバッチを構成せよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101, 2200, 6057,  ...,    0,    0,    0],\n",
      "        [ 101, 2028, 2062,  ...,    0,    0,    0],\n",
      "        [ 101, 1011, 1011,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 1996, 2832,  ...,    0,    0,    0],\n",
      "        [ 101, 2036, 1037,  ...,    0,    0,    0],\n",
      "        [ 101, 2589, 1010,  ...,    0,    0,    0]])\n"
     ]
    }
   ],
   "source": [
    "collate_fn = DataCollatorWithPadding(tokenizer, return_tensors=\"pt\")\n",
    "train_dl = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "valid_dl = DataLoader(valid_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print(next(iter(train_dl))['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 87. ファインチューニング\n",
    "訓練セットを用い、事前学習済みモデルを極性分析タスク向けにファインチューニングせよ。検証セット上でファインチューニングされたモデルの正解率を計測せよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 — Avg Training Loss: 0.2575\n",
      "Epoch 1 — Validation Accuracy: 0.9209\n",
      "\n",
      "Epoch 2 — Avg Training Loss: 0.1133\n",
      "Epoch 2 — Validation Accuracy: 0.9209\n",
      "\n",
      "Epoch 3 — Avg Training Loss: 0.0702\n",
      "Epoch 3 — Validation Accuracy: 0.9220\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.nn.functional import cross_entropy\n",
    "from transformers import BertForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels=2\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "epochs = 3\n",
    "total_steps = len(train_dl) * epochs\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps = int(0.1 * total_steps),\n",
    "    num_training_steps = total_steps\n",
    ")\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    losses = []\n",
    "\n",
    "    for batch in train_dl:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = sum(losses) / len(losses)\n",
    "    print(f\"Epoch {epoch} — Avg Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_dl:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().tolist())\n",
    "            all_labels.extend(batch['labels'].tolist())\n",
    "    \n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    print(f\"Epoch {epoch} — Validation Accuracy: {acc:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 88. 極性分析\n",
    "問題87でファインチューニングされたモデルを用いて、以下の文の極性を予測せよ。\n",
    "\n",
    "“The movie was full of incomprehensibilities.”\n",
    "\n",
    "“The movie was full of fun.”\n",
    "\n",
    "“The movie was full of excitement.”\n",
    "\n",
    "“The movie was full of crap.”\n",
    "\n",
    "“The movie was full of rubbish.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"The movie was full of incomprehensibilities.\"\n",
      "  → Predicted: Negative\n",
      "\n",
      "\"The movie was full of fun.\"\n",
      "  → Predicted: Positive\n",
      "\n",
      "\"The movie was full of excitement.\"\n",
      "  → Predicted: Positive\n",
      "\n",
      "\"The movie was full of crap.\"\n",
      "  → Predicted: Negative\n",
      "\n",
      "\"The movie was full of rubbish.\"\n",
      "  → Predicted: Negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    \"The movie was full of incomprehensibilities.\",\n",
    "    \"The movie was full of fun.\",\n",
    "    \"The movie was full of excitement.\",\n",
    "    \"The movie was full of crap.\",\n",
    "    \"The movie was full of rubbish.\"\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for text in texts:\n",
    "        inputs = tokenizer(text, return_tensors='pt').to(device)\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits[0]  # (, 2)\n",
    "\n",
    "        pred_id = torch.argmax(logits).item()\n",
    "\n",
    "        label_map = {0: \"Negative\", 1: \"Positive\"}\n",
    "        print(f\"\\\"{text}\\\"\")\n",
    "        print(f\"  → Predicted: {label_map[pred_id]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 89. アーキテクチャの変更\n",
    "問題87とは異なるアーキテクチャ（例えば[CLS]トークンを用いるか、各トークンの最大値プーリングを用いるなど）の分類モデルを設計し、事前学習済みモデルを極性分析タスク向けにファインチューニングせよ。検証セット上でファインチューニングされたモデルの正解率を計測せよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 — Avg Training Loss: 0.2492\n",
      "Epoch 1 — Validation Accuracy: 0.9209\n",
      "\n",
      "Epoch 2 — Avg Training Loss: 0.1083\n",
      "Epoch 2 — Validation Accuracy: 0.9174\n",
      "\n",
      "Epoch 3 — Avg Training Loss: 0.0679\n",
      "Epoch 3 — Validation Accuracy: 0.9163\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Use the averge of the last hidden state of the fine-tuned Bert model to predict.\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "class MyBertForBinaryClassification(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Load the pre-trained Bert model\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        # Get the hidden size so that we can set the size of fully connected layer\n",
    "        hidden_size = self.bert.config.hidden_size\n",
    "        # For 2 classification task\n",
    "        self.fc = nn.Linear(hidden_size, 2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
    "        h = outputs.last_hidden_state   # [batch_size, seq_len, hidden_size]\n",
    "        # Omit the [CLS] and [SEP] token\n",
    "        h = h[:, 1:-1, :]   # [batch_size, seq_len - 2, hidden_size]\n",
    "\n",
    "        mask = attention_mask[:, 1:-1].unsqueeze(-1) # [batch_size, seq_len, 1] for broadcasting\n",
    "        vec = (h * mask).sum(1) / mask.sum(1)   # [batch_size, hidden_size]\n",
    "        logits = self.fc(vec)\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        return {'loss': loss, 'logits': logits}\n",
    "\n",
    "\n",
    "model_89 = MyBertForBinaryClassification().to(device)\n",
    "\n",
    "epochs = 3\n",
    "total_steps = len(train_dl) * epochs\n",
    "optimizer = AdamW(model_89.parameters(), lr=2e-5)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps = int(0.1 * total_steps),\n",
    "    num_training_steps = total_steps\n",
    ")\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model_89.train()\n",
    "    losses = []\n",
    "\n",
    "    for batch in train_dl:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model_89(**batch)\n",
    "        loss = outputs['loss']\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    avg_train_loss = sum(losses) / len(losses)\n",
    "    print(f\"Epoch {epoch} — Avg Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # Evaluation\n",
    "    model_89.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_dl:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model_89(**batch)\n",
    "\n",
    "            logits = outputs['logits']\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().tolist())\n",
    "            all_labels.extend(batch['labels'].cpu().tolist())\n",
    "    \n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    print(f\"Epoch {epoch} — Validation Accuracy: {acc:.4f}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp100-py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
