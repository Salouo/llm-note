{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第10章: 事前学習済み言語モデル（GPT型）\n",
    "本章では、GPT型（Transformerのデコーダ型）の事前学習済みモデルを利用して、言語生成、評判分析器（ポジネガ分類器）の構築、ファインチューニング、強化学習などに取り組む。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 90. 次単語予測\n",
    "“The movie was full of”に続くトークン（トークン列ではなく一つのトークンであることに注意せよ）として適切なもの上位10個と、その確率（尤度）を求めよ。ただし、言語モデルへのプロンプトがどのようなトークン列に変換されたか、確認せよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: ['The', 'Ġmovie', 'Ġwas', 'Ġfull', 'Ġof']\n",
      "token_ids: [464, 3807, 373, 1336, 286]\n",
      "      Ġjokes (id=14532): 0.0219\n",
      "      Ġgreat (id=1049): 0.0186\n",
      "     Ġlaughs (id=22051): 0.0115\n",
      "        Ġbad (id=2089): 0.0109\n",
      "  Ġsurprises (id=24072): 0.0107\n",
      " Ġreferences (id=10288): 0.0105\n",
      "        Ġfun (id=1257): 0.0100\n",
      "      Ġhumor (id=14733): 0.0074\n",
      "          Ġ\" (id=366): 0.0074\n",
      "        Ġthe (id=262): 0.0067\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.eval()\n",
    "\n",
    "prompt = 'The movie was full of'\n",
    "tokens = tokenizer.tokenize(prompt)\n",
    "# tokens -> ids\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(f\"tokens: {tokens}\")\n",
    "print(f\"token_ids: {ids}\")\n",
    "\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt')   # [1, seq_len]\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "    logits = outputs.logits # [1, seq_len, vocab_size]\n",
    "\n",
    "# Obtain the logits at the last position\n",
    "last_logits = logits[0, -1, :]\n",
    "probs = F.softmax(last_logits, dim=-1)\n",
    "\n",
    "topk = torch.topk(probs, k=10)\n",
    "top_ids = topk.indices.tolist()\n",
    "top_probs = topk.values.tolist()\n",
    "\n",
    "# ids -> tokens\n",
    "top_tokens = tokenizer.convert_ids_to_tokens(top_ids)\n",
    "for tok, pid, p in zip(top_tokens, top_ids, top_probs):\n",
    "    print(f\"{tok:>12s} (id={pid}): {p:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 91. 続きのテキストの予測\n",
    "“The movie was full of”に続くテキストを複数予測せよ。このとき、デコーディングの方法や温度パラメータ（temperature）を変えながら、予測される複数のテキストの変化を観察せよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Greedy Search ===\n",
      "['The movie was full of jokes and jokes about how']\n",
      "\n",
      "=== Beam Search (beams=5) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The movie was full of jokes and jokes and jokes']\n",
      "\n",
      "=== Sampling, temperature=0.7 ===\n",
      "['The movie was full of jokes, from the ridiculous']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model.eval()\n",
    "\n",
    "prompt = \"The movie was full of\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")  # shape [1, seq_len]\n",
    "\n",
    "settings = [\n",
    "    {\n",
    "        'name': 'Greedy Search',\n",
    "        'generate_kwargs': {\n",
    "            'max_length': input_ids.shape[1] + 5,\n",
    "            'do_sample': False\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Beam Search (beams=5)\",\n",
    "        \"generate_kwargs\": {\n",
    "            \"max_length\": input_ids.shape[1] + 5,\n",
    "            \"num_beams\": 5,\n",
    "            \"early_stopping\": True,\n",
    "            \"do_sample\": False,\n",
    "            \"num_return_sequences\": 1\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Sampling, temperature=0.7\",\n",
    "        \"generate_kwargs\": {\n",
    "            \"max_length\": input_ids.shape[1] + 5,\n",
    "            \"do_sample\": True,\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_k\": 50,\n",
    "            \"num_return_sequences\": 1\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "for setting in settings:\n",
    "    # Greedy Search / Beam Search / Sampling (temperature=0.7)\n",
    "    print(f\"\\n=== {setting['name']} ===\")\n",
    "    outputs = model.generate(input_ids, **setting['generate_kwargs'])\n",
    "    texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    print(texts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 92. 予測されたテキストの確率を計算\n",
    "“The movie was full of”に続くテキストを予測し、生成された各単語の尤度を表示せよ（生成されるテキストが長いと出力が読みにくくなるので、適当な長さで生成を打ち切るとよい）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 | Ġjokes          | 2.19%\n",
      "6 | Ġand            | 28.92%\n",
      "7 | Ġjokes          | 9.85%\n",
      "8 | Ġabout          | 20.56%\n",
      "9 | Ġhow            | 9.97%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model     = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model.eval()\n",
    "\n",
    "# inputs\n",
    "prompt   = \"The movie was full of\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids  # [1, seq_len]\n",
    "all_tokens_len = input_ids.shape[1] + 5\n",
    "\n",
    "# \n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_length=all_tokens_len,\n",
    "    do_sample=False,\n",
    "    output_scores=True,\n",
    "    return_dict_in_generate=True    # return the dict-like output\n",
    ")\n",
    "\n",
    "generate_ids = outputs.sequences[0] # outputs.shape: [batch_size, seq_len]\n",
    "scores = outputs.scores # [generate_len, batch_size, vocab_size]\n",
    "\n",
    "all_tokens = tokenizer.convert_ids_to_tokens(generate_ids.tolist())\n",
    "\n",
    "for i, score in enumerate(scores):\n",
    "    probs = F.softmax(score[0], dim=-1) # score: [batch_size, vocab_size]\n",
    "    cur_token_id = generate_ids[input_ids.shape[1] + i].item()\n",
    "    token  = all_tokens[input_ids.shape[1] + i]\n",
    "    p = probs[cur_token_id].item()\n",
    "    print(f\"{input_ids.shape[1]+i} | {token:<15s} | {p * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 93. パープレキシティ\n",
    "適当な文を準備して、事前学習済み言語モデルでパープレキシティを測定せよ。例えば、\n",
    "\n",
    "- The movie was full of surprises\n",
    "- The movies were full of surprises\n",
    "- The movie were full of surprises\n",
    "- The movies was full of surprises\n",
    "\n",
    "の4文に対して、パープレキシティを測定して観察せよ（最後の2つの文は故意に文法的な間違いを入れた）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The movie was full of surprisesperplexity: 99.3547430505594\n",
      "The movies were full of surprisesperplexity: 126.48318169449198\n",
      "The movie were full of surprisesperplexity: 278.88188091019293\n",
      "The movies was full of surprisesperplexity: 274.6604853196633\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import math\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.eval()\n",
    "\n",
    "sentences = [\n",
    "    \"The movie was full of surprises\",\n",
    "    \"The movies were full of surprises\",\n",
    "    \"The movie were full of surprises\",\n",
    "    \"The movies was full of surprises\"\n",
    "]\n",
    "\n",
    "for s in sentences:\n",
    "    inputs = tokenizer(s, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs['input_ids'])\n",
    "        loss = outputs.loss\n",
    "        # denifition of perplexity: exp(CrossEntropy)\n",
    "        perplexity = math.exp(loss)\n",
    "    print(f\"{s}perplexity: {perplexity}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 94. チャットテンプレート\n",
    "“What do you call a sweet eaten after dinner?”という問いかけに対する応答を生成するため、チャットテンプレートを適用し、言語モデルに与えるべきプロンプトを作成せよ。また、そのプロンプトに対する応答を生成し、表示せよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You are a helpful, concise assistant that answers language‐related questions.\n",
      "User: What do you call a sweet eaten after dinner?\n",
      "Assistant: A sweet eaten after dinner.\n",
      "User: What do you call a sweet eaten after dinner?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "tokenizer  = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model      = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "# GPT series prompt templeate\n",
    "prompt_text = (\n",
    "    \"System: You are a helpful, concise assistant that answers language‐related questions.\\n\"\n",
    "    \"User: What do you call a sweet eaten after dinner?\\n\"\n",
    "    \"Assistant:\"\n",
    ")\n",
    "\n",
    "inputs = tokenizer(prompt_text, return_tensors='pt')\n",
    "input_len = inputs['input_ids'].shape[-1]\n",
    "\n",
    "output_ids = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=20,\n",
    "    do_sample=False     # Greedy search\n",
    ")\n",
    "\n",
    "# Obtain new generated tokens\n",
    "gen_ids = output_ids[0, input_len:] # [batch_size, all_tokens_length]\n",
    "\n",
    "# Decode\n",
    "generated = tokenizer.decode(gen_ids, skip_special_tokens=True) # new generated text\n",
    "print(prompt_text + generated)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 95. マルチターンのチャット\n",
    "問題94で生成された応答に対して、追加で”Please give me the plural form of the word with its spelling in reverse order.”と問いかけたときの応答を生成・表示せよ。また、その時に言語モデルに与えるプロンプトを確認せよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You are a helpful, concise assistant that answers language-related questions.\n",
      "User: What do you call a sweet eaten after dinner?\n",
      "Assistant: A sweet eaten after dinner.\n",
      "User: Please give me the plural form of the word with its spelling in reverse order.\n",
      "Assistant: I am a sweet eaten after dinner.\n",
      "User: Please give me the plural form of the word\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "tokenizer  = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model      = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "prompt_text = (\n",
    "    \"System: You are a helpful, concise assistant that answers language-related questions.\\n\"\n",
    "    \"User: What do you call a sweet eaten after dinner?\\n\"\n",
    "    \"Assistant: A sweet eaten after dinner.\\n\"\n",
    "    \"User: Please give me the plural form of the word with its spelling in reverse order.\\n\"\n",
    "    \"Assistant:\"\n",
    ")\n",
    "\n",
    "inputs    = tokenizer(prompt_text, return_tensors=\"pt\")\n",
    "prompt_len = inputs['input_ids'].shape[-1]\n",
    "\n",
    "output_ids = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=20,\n",
    "    do_sample=False\n",
    ")\n",
    "\n",
    "gen_ids   = output_ids[0, prompt_len:]\n",
    "generated = tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
    "\n",
    "print(prompt_text + generated)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 96. プロンプトによる感情分析\n",
    "事前学習済み言語モデルで感情分析を行いたい。テキストを含むプロンプトを事前学習済み言語モデルに与え、（ファインチューニングは行わずに）テキストのポジネガを予測するという戦略で、SST-2の開発データにおける正解率を測定せよ。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 63.07%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "tokenizer  = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model_96 = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model_96.eval()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_96.to(device)\n",
    "\n",
    "valid_df = pd.read_csv('dev.tsv', sep='\\t')\n",
    "sentences = valid_df['sentence'].tolist()\n",
    "labels = np.asarray(valid_df['label'].astype(int).tolist())\n",
    "\n",
    "\n",
    "def predict_sentiment(sentence: str) -> int:\n",
    "    prompt = sentence + \"\\nSentiment: \"\n",
    "    sentiments = [\"positive\", \"negative\"]\n",
    "    judge = []\n",
    "    for sentiment in sentiments:\n",
    "        inputs = tokenizer(prompt + sentiment, return_tensors='pt').to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model_96(**inputs, labels=inputs['input_ids'])\n",
    "            judge.append(outputs.loss.item())\n",
    "    # If the loss of `positive` is less, return 1; else return 0\n",
    "    if judge[0] < judge[1]:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "correct, total = 0, len(valid_df)\n",
    "\n",
    "preds = []\n",
    "for sentence in sentences:\n",
    "    preds.append(predict_sentiment(sentence))\n",
    "preds = np.asarray(preds)\n",
    "\n",
    "accuracy = np.mean(preds == labels)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 97. 埋め込みに基づく感情分析\n",
    "事前学習済み言語モデルでテキストをベクトルで表現（エンコード）し、そのベクトルにフィードフォワード層を通すことで極性ラベルを予測するモデルを学習せよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of gpus: 2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2,3\"\n",
    "\n",
    "n_gpus = torch.cuda.device_count()\n",
    "print(f\"Number of gpus: {n_gpus}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junfeng-c/miniconda3/envs/nlp100-py310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 ⇒ training loss 0.3337 | validation acc 90.02%\n",
      "\n",
      "Epoch 2 ⇒ training loss 0.2007 | validation acc 90.37%\n",
      "\n",
      "Epoch 3 ⇒ training loss 0.1619 | validation acc 91.28%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, GPT2Model, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class SSTDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.texts = df['sentence'].tolist()\n",
    "        self.labels = df['label'].astype(int).tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        labels = self.labels[idx]\n",
    "        texts = self.texts[idx]\n",
    "        enc = self.tokenizer(\n",
    "            texts,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        enc['labels'] = torch.tensor(labels, dtype=torch.long)\n",
    "        enc = {k: v.squeeze(0) for k, v in enc.items()}  \n",
    "        return enc\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "train_df = pd.read_csv('train.tsv', sep='\\t')\n",
    "valid_df = pd.read_csv('dev.tsv', sep='\\t')\n",
    "\n",
    "train_ds = SSTDataset(train_df, tokenizer)\n",
    "valid_ds = SSTDataset(valid_df, tokenizer)\n",
    "train_dl = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=64, shuffle=False)\n",
    "\n",
    "class MyGPT2Sentiment(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Load the pre-trained GPT-2\n",
    "        self.gpt2 = GPT2Model.from_pretrained('gpt2')\n",
    "        hidden_size = self.gpt2.config.hidden_size\n",
    "        # FFN layer for binary classification\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size // 2, 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.gpt2(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = outputs.last_hidden_state # [batch_size, seq_len, hidden_size]\n",
    "        mask = attention_mask.unsqueeze(-1) # [batch_size, seq_len, 1] for broadcasting\n",
    "        masked_sum = (last_hidden_state * mask).sum(dim=1)  # [batch_size, hidden_size]\n",
    "        lengths = mask.sum(dim=1).clamp(min=1e-6)\n",
    "        pooled = masked_sum / lengths\n",
    "        out = self.classifier(pooled)\n",
    "        return out\n",
    "\n",
    "# number of epochs\n",
    "epochs = 3\n",
    "# Initialize my model\n",
    "my_model = MyGPT2Sentiment()\n",
    "\n",
    "# Try to use more than one gpu\n",
    "if torch.cuda.device_count() > 1:\n",
    "    my_model = nn.DataParallel(my_model)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "my_model.to(device)\n",
    "\n",
    "# training settings\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(my_model.parameters(), lr=2e-5)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, 0, len(train_dl) * epochs\n",
    ")\n",
    "\n",
    " # ---- Process ----\n",
    "for epoch in range(1, epochs + 1):\n",
    "    # ---- Train ----\n",
    "    my_model.train()\n",
    "    running = 0\n",
    "    for batch in train_dl:\n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Put the data on device(gpu)\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        '''\n",
    "        input_ids      = batch[\"input_ids\"]      # [B, L]\n",
    "        attention_mask = batch[\"attention_mask\"] # [B, L]\n",
    "        labels         = batch[\"labels\"]         # [B]\n",
    "        '''\n",
    "        logits = my_model(batch['input_ids'], batch['attention_mask'])\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        loss = loss_fn(logits, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        running += loss.item()\n",
    "\n",
    "    # ---- Eval ----\n",
    "    my_model.eval()\n",
    "    correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_dl:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            preds = my_model(batch['input_ids'], batch['attention_mask']).argmax(dim=-1)\n",
    "            labels = batch['labels'].to(device)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    acc = 100 * correct / total\n",
    "    print(f\"Epoch {epoch} ⇒ training loss {running/len(train_dl):.4f} | validation acc {acc:.2f}%\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 98. ファインチューニング\n",
    "問題96のプロンプトに対して、正解の感情ラベルをテキストの応答として返すように事前学習済みモデルをファインチューニングせよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of gpus: 3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2,3\"\n",
    "\n",
    "n_gpus = torch.cuda.device_count()\n",
    "print(f\"Number of gpus: {n_gpus}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junfeng-c/miniconda3/envs/nlp100-py310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
      "/home/junfeng-c/miniconda3/envs/nlp100-py310/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "training loss: 0.030596\n",
      "validation loss: 0.326413\n",
      "validation acc: 0.871560\n",
      "Epoch 2\n",
      "training loss: 0.005133\n",
      "validation loss: 0.282930\n",
      "validation acc: 0.888761\n",
      "Epoch 3\n",
      "training loss: 0.004401\n",
      "validation loss: 0.256442\n",
      "validation acc: 0.894495\n",
      "Epoch 4\n",
      "training loss: 0.004078\n",
      "validation loss: 0.245443\n",
      "validation acc: 0.897936\n",
      "Epoch 5\n",
      "training loss: 0.003781\n",
      "validation loss: 0.251541\n",
      "validation acc: 0.891055\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_96 = GPT2LMHeadModel.from_pretrained('gpt2').to(device)\n",
    "# Try to use more than one gpu\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model_96 = nn.DataParallel(model_96)\n",
    "\n",
    "# Load data\n",
    "train_df = pd.read_csv(\"train.tsv\", sep=\"\\t\")\n",
    "valid_df = pd.read_csv(\"dev.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Map 1 -> pos; 0 -> neg\n",
    "id2label = {1: \"positive\", 0: \"negative\"}\n",
    "train_df[\"target\"] = train_df[\"label\"].map(id2label)\n",
    "valid_df[\"target\"] = valid_df[\"label\"].map(id2label)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# customized dataset\n",
    "class SentimentPromptDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=1024):\n",
    "        self.sentences = df[\"sentence\"].tolist()\n",
    "        self.answers = df[\"target\"].tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        prompt = f\"{self.sentences[index]}\\nSentiment: \"\n",
    "        answer = self.answers[index]\n",
    "\n",
    "        # Encode prompt and answer, respetively\n",
    "        prompt_ids = self.tokenizer(prompt, add_special_tokens=False).input_ids\n",
    "        answer_ids = self.tokenizer(answer, add_special_tokens=False).input_ids\n",
    "\n",
    "        # If length of prompt is larger than that of max length for prompt, cur the front part of prompt\n",
    "        max_prompt = self.max_length - len(answer_ids)\n",
    "        if len(prompt_ids) > max_prompt:\n",
    "            prompt_ids = prompt_ids[-max_prompt:]\n",
    "\n",
    "        # Concatenate `prompt` and `answer`\n",
    "        ids = torch.tensor(prompt_ids + answer_ids, dtype=torch.long)\n",
    "\n",
    "        # During training, the model computes logits for every token in the sequence, \n",
    "        # but we only need (and use) the logits corresponding to the answer tokens.\n",
    "        labels = ids.clone()\n",
    "        labels[:len(prompt_ids)] = -100\n",
    "\n",
    "        assert (labels != -100).any(), f\"All labels are -100 at idx={index}\"\n",
    "\n",
    "        return ids, labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "def collate_fn(batch):\n",
    "    ids, labels = zip(*batch)   # [id_01, id_02, ...]; [label_01, label_02, ...]\n",
    "\n",
    "    ids = torch.nn.utils.rnn.pad_sequence(ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "    # Mask the padding token\n",
    "    attn_mask = (ids != tokenizer.pad_token_id)\n",
    "    return {\n",
    "        \"input_ids\": ids.to(device),\n",
    "        \"attention_mask\": attn_mask.to(device),\n",
    "        \"labels\": labels.to(device)\n",
    "    }\n",
    "\n",
    "train_dl = DataLoader(SentimentPromptDataset(train_df, tokenizer), batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "valid_dl = DataLoader(SentimentPromptDataset(valid_df, tokenizer), batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# ----Train `model_96`----\n",
    "num_epochs = 5\n",
    "optimizer = AdamW(model_96.parameters(), lr=6e-6)  # Keep lienar increment with batch size\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=len(train_dl), num_training_steps=len(train_dl) * num_epochs\n",
    ")\n",
    "\n",
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    losses, preds = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_dl:\n",
    "            out = model(**batch)\n",
    "            batch_size = batch[\"input_ids\"].size(0)\n",
    "            batch_total_loss = out.loss.mean().item() * batch_size  # Use more than one gpu; `[n_gpus].mean()`\n",
    "            losses.append(batch_total_loss)\n",
    "\n",
    "            input_texts = tokenizer.batch_decode(batch[\"input_ids\"], skip_special_tokens=True)\n",
    "\n",
    "            for txt in input_texts:\n",
    "                prompt = txt.split(\"Sentiment: \")[0] + \"Sentiment: \"\n",
    "                pos_ids = tokenizer(prompt + \"positive\", return_tensors=\"pt\").to(device)\n",
    "                neg_ids = tokenizer(prompt + \"negative\", return_tensors=\"pt\").to(device)\n",
    "                pos_loss = model(**pos_ids, labels=pos_ids[\"input_ids\"]).loss.item()\n",
    "                neg_loss = model(**neg_ids, labels=neg_ids[\"input_ids\"]).loss.item()\n",
    "                preds.append(1 if pos_loss < neg_loss else 0)\n",
    "        valid_acc = np.mean(np.array(preds) == np.array(valid_df['label'].tolist()))\n",
    "        valid_loss = sum(losses) / len(valid_df)\n",
    "        return valid_loss , valid_acc\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    model_96.train()\n",
    "    total_train_loss= 0\n",
    "    for batch in train_dl:\n",
    "        loss = model_96(**batch).loss\n",
    "        # Use more than one gpu\n",
    "        if loss.dim() > 0:\n",
    "            loss = loss.mean()\n",
    "\n",
    "        # Check if the gradient exploding problem occurs\n",
    "        if torch.isnan(loss):\n",
    "            print(\"NaN comes directly from forward pass\")\n",
    "            break\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    valid_loss, valid_acc = evaluate(model_96)\n",
    "\n",
    "    \n",
    "    print(f\"Epoch {epoch}\")\n",
    "    print(f\"training loss: {total_train_loss / len(train_df):.6f}\")\n",
    "    print(f\"validation loss: {valid_loss:.6f}\")\n",
    "    print(f\"validation acc: {valid_acc:.6f}\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 99. 選好チューニング\n",
    "問題96のプロンプトに対して、正解の感情ラベルを含むテキストを望ましい応答、間違った感情ラベルを含むテキストを望ましくない応答として、事前学習済み言語モデルを選好チューニング (preference tuning) を実施せよ。選好チューニングのアルゴリズムとしては、近傍方策最適化 (PPO: Proximal Policy Optimization) や直接選好最適化 (DPO: Direct Preference Optimization) などが考えられる。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp100-py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
