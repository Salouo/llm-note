{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "555c5ee2",
   "metadata": {},
   "source": [
    "# Chapter06: Advanced RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4114e0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "loaded = load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e4cf37",
   "metadata": {},
   "source": [
    "## 6.1 Simple RAG Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71a524ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "419\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import GitLoader\n",
    "\n",
    "def file_filter(file_path: str) -> bool:\n",
    "    return file_path.endswith(\".mdx\")\n",
    "\n",
    "loader = GitLoader(\n",
    "    clone_url=\"https://github.com/langchain-ai/langchain\",\n",
    "    repo_path=\"./langchain\",\n",
    "    branch=\"master\",\n",
    "    file_filter=file_filter\n",
    ")\n",
    "\n",
    "documents = loader.load()\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d02278d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Load the embedding model\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "# Vectorize text\n",
    "db = Chroma.from_documents(documents, embeddings)\n",
    "# Instantiate a retriever\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8e1e4e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChainは、大規模言語モデル（LLMs）を活用したアプリケーションを開発するためのフレームワークです。LangChainは、LLMアプリケーションのライフサイクルの各段階を簡素化します。具体的には、開発、プロダクション化、デプロイメントの各ステージで役立ちます。\\n\\n- **開発**: LangChainのオープンソースコンポーネントやサードパーティの統合を利用してアプリケーションを構築できます。また、LangGraphを使用して、ストリーミングや人間の介入をサポートする状態保持エージェントを構築できます。\\n- **プロダクション化**: LangSmithを使用してアプリケーションを検査、監視、評価し、継続的に最適化して自信を持ってデプロイできます。\\n- **デプロイメント**: LangGraph Platformを使用して、LangGraphアプリケーションをプロダクション対応のAPIやアシスタントに変換できます。\\n\\nLangChainは、LLMsや関連技術（埋め込みモデルやベクトルストアなど）の標準インターフェースを実装し、数百のプロバイダーと統合しています。これにより、開発者は異なるプロバイダー間での切り替えが容易になり、複数のコンポーネントを組み合わせた複雑なアプリケーションの構築が可能になります。'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template('''\\\n",
    "以下の文脈だけを踏まえて質問に回答してください。\n",
    "\n",
    "文脈：\"\"\"\n",
    "{context}\n",
    "\"\"\"\n",
    "                                \n",
    "質問：{question}\n",
    "''')\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "chain = {\n",
    "    \"question\": RunnablePassthrough(),\n",
    "    \"context\": retriever,\n",
    "} | prompt | model | StrOutputParser()\n",
    "\n",
    "chain.invoke(\"LangChainの概要を教えて\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2065de",
   "metadata": {},
   "source": [
    "## 6.2 HyDE (Hypothetical Document Embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464b3f66",
   "metadata": {},
   "source": [
    "We want to retrieve contexts that are relevant to the model’s answer rather than to the query. Therefore, we **first prompt the LLM to generate a hypothetical**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a414dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template('''\\\n",
    "以下の文脈だけを踏まえて質問に回答してください。\n",
    "\n",
    "文脈：\"\"\"\n",
    "{context}\n",
    "\"\"\"\n",
    "                                \n",
    "質問：{question}\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0c2a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothetical_prompt = ChatPromptTemplate.from_template(\"\"\"\\\n",
    "次の質問に回答する一文を書いてください。\n",
    "\n",
    "質問：{question}\n",
    "\"\"\")\n",
    "\n",
    "hypothetical_chain = hypothetical_prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9acc74c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChainは、大規模言語モデル（LLMs）を活用したアプリケーションを開発するためのフレームワークです。以下のような特徴があります：\\n\\n1. **開発の簡素化**: LangChainは、オープンソースのコンポーネントやサードパーティの統合を利用して、アプリケーションの開発を簡素化します。LangGraphを使用して、状態を持つエージェントを構築し、ストリーミングや人間の介入をサポートします。\\n\\n2. **プロダクション化**: LangSmithを使用して、アプリケーションを検査、監視、評価することで、継続的に最適化し、自信を持ってデプロイできます。\\n\\n3. **デプロイメント**: LangGraphプラットフォームを使用して、LangGraphアプリケーションをプロダクション対応のAPIやアシスタントに変換します。\\n\\nLangChainは、LLMsや関連技術（埋め込みモデルやベクトルストアなど）の標準インターフェースを実装し、数多くのプロバイダーと統合しています。これにより、開発者は異なるプロバイダー間での切り替えが容易になり、複雑なアプリケーションのオーケストレーションや観察性、評価をサポートします。'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyde_rag_chain = {\n",
    "    \"question\": RunnablePassthrough(),\n",
    "    \"context\": hypothetical_chain | retriever,  # The hypothetical will be sent to `prompt`\n",
    "} | prompt | model | StrOutputParser()\n",
    "\n",
    "hyde_rag_chain.invoke(\"LangChainの概要を教えて\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11137f61",
   "metadata": {},
   "source": [
    "We can also guide model to generate multiple hypotheticals (queries), which can be utilized to retrieve relevant documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5561ac61",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "`BaseModel` is used to declare exactly what shape the model's output should take.\n",
    "`Field` is used to regularize the properties in `BaseModel`.\n",
    "'''\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class QueryGenerationOutput(BaseModel):\n",
    "    queries: list[str] = Field(..., description=\"検索クエリのリスト\")\n",
    "\n",
    "query_generation_prompt = ChatPromptTemplate.from_template(\"\"\"\\\n",
    "質問に対してベクターデータベースから関連文書を検索するために、\n",
    "三つの異なる検索クエリを生成してください。\n",
    "距離ベースの類似性検索の限界を克服するために、\n",
    "ユーザーの質問に対して複数の視点を提供することが目標です。\n",
    "                                                           \n",
    "質問：{question}\n",
    "\"\"\")\n",
    "\n",
    "query_generation_chain = (\n",
    "    query_generation_prompt\n",
    "    | model.with_structured_output(QueryGenerationOutput)   # Fix the format of output\n",
    "    | (lambda x: x.queries)     # Get `queries` from `QueryGenerationOutput` object; final output will be a list[str]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f02d26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChainは、大規模言語モデル（LLMs）を活用したアプリケーションを開発するためのフレームワークです。LangChainは、LLMアプリケーションのライフサイクル全体を簡素化することを目的としています。具体的には、以下の3つのステージで役立ちます：\\n\\n1. **開発**: LangChainのオープンソースコンポーネントやサードパーティの統合を利用してアプリケーションを構築できます。また、LangGraphを使用して、ストリーミングや人間の介入をサポートする状態を持つエージェントを構築できます。\\n\\n2. **プロダクション化**: LangSmithを使用して、アプリケーションを検査、監視、評価することで、継続的に最適化し、自信を持ってデプロイできます。\\n\\n3. **デプロイメント**: LangGraphアプリケーションをプロダクション対応のAPIやアシスタントに変換できます。\\n\\nLangChainは、チャットモデルや埋め込みモデル、ベクトルストアなどの大規模言語モデルと関連技術のための標準インターフェースを実装し、数多くのプロバイダーと統合しています。'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_query_rag_chain = (\n",
    "    {\n",
    "    \"question\": RunnablePassthrough(),\n",
    "    \"context\": query_generation_chain | retriever.map(), # `retriever.map()` is used to receive a list; list[str] -> list[list[Document]]\n",
    "    }\n",
    "    | prompt | model | StrOutputParser()\n",
    ")\n",
    "multi_query_rag_chain.invoke(\"LangChainの概要を教えて\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278de911",
   "metadata": {},
   "source": [
    "We can also use `RAG-Fusion` menthod to determine the rank of retrieved context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382005ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "def reciprocal_rank_fusion(retriever_outputs: list[list[Document]], k: int = 60) -> list[str]:\n",
    "    # `{content: score} dict`` for ranking; rank the content by coresponding the score\n",
    "    content_score_mapping = {}\n",
    "\n",
    "    for docs in retriever_outputs:\n",
    "        for rank, doc in enumerate(docs): \n",
    "            content = doc.page_content\n",
    "            # If this context occurs firstly, add it into the `dict`\n",
    "            if content not in content_score_mapping:\n",
    "                content_score_mapping[content] = 0\n",
    "            # accumulate score corresponding to the current context\n",
    "            content_score_mapping[content] += 1 / (rank + k)\n",
    "    # rank each context by their accumulated scores\n",
    "    ranked = sorted(content_score_mapping.items(), key=lambda x: x[1], reverse=True)\n",
    "    return [content for content, _ in ranked]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a5fc3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChainは、大規模言語モデル（LLMs）を活用したアプリケーションを開発するためのフレームワークです。LangChainは、LLMアプリケーションのライフサイクル全体を簡素化することを目的としています。具体的には、以下のような機能を提供しています：\\n\\n1. **開発**: LangChainのオープンソースコンポーネントやサードパーティの統合を利用してアプリケーションを構築できます。また、LangGraphを使用して、ストリーミングや人間の介入をサポートするエージェントを構築することができます。\\n\\n2. **プロダクション化**: LangSmithを使用して、アプリケーションを検査、監視、評価することで、継続的に最適化し、自信を持ってデプロイすることができます。\\n\\n3. **デプロイ**: LangGraphアプリケーションをプロダクション対応のAPIやアシスタントに変換することができます。\\n\\nLangChainは、チャットモデルや埋め込みモデル、ベクトルストアなどの関連技術のための標準インターフェースを実装しており、数多くのプロバイダーと統合されています。これにより、開発者は異なるプロバイダー間で簡単に切り替えたり、コンポーネントを組み合わせたりすることができます。'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_fusion_chain = {\n",
    "    \"question\": RunnablePassthrough(),\n",
    "    \"context\": query_generation_chain | retriever.map() | reciprocal_rank_fusion,\n",
    "} | prompt | model | StrOutputParser()\n",
    "\n",
    "rag_fusion_chain.invoke(\"LangChainの概要を教えて\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f16f3c8",
   "metadata": {},
   "source": [
    "## 6.3 Cohere"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398fb95b",
   "metadata": {},
   "source": [
    "### `Cohere` can be used to rerank documents corresponding to one query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9595d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "419\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import GitLoader\n",
    "\n",
    "def file_filter(file_path: str) -> bool:\n",
    "    return file_path.endswith(\".mdx\")\n",
    "\n",
    "loader = GitLoader(\n",
    "    clone_url=\"https://github.com/langchain-ai/langchain\",\n",
    "    repo_path=\"./langchain\",\n",
    "    branch=\"master\",\n",
    "    file_filter=file_filter\n",
    ")\n",
    "\n",
    "documents = loader.load()\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6040e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Load the embedding model\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "# Vectorize text\n",
    "db = Chroma.from_documents(documents, embeddings)\n",
    "# Instantiate a retriever\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943a0717",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template('''\\\n",
    "以下の文脈だけを踏まえて質問に回答してください。\n",
    "\n",
    "文脈：\"\"\"\n",
    "{context}\n",
    "\"\"\"\n",
    "                                \n",
    "質問：{question}\n",
    "''')\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd0f774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChainは、大規模言語モデル（LLMs）を活用したアプリケーションを開発するためのフレームワークです。このフレームワークは、LLMアプリケーションのライフサイクルの各段階を簡素化します。具体的には、以下のような機能を提供しています：\\n\\n1. **開発**: LangChainのオープンソースコンポーネントやサードパーティの統合を利用してアプリケーションを構築できます。また、LangGraphを使用して、ストリーミングや人間の介入をサポートする状態を持つエージェントを構築できます。\\n\\n2. **プロダクション化**: LangSmithを使用してアプリケーションを検査、監視、評価し、継続的に最適化して自信を持ってデプロイできます。\\n\\n3. **デプロイメント**: LangGraph Platformを使用して、LangGraphアプリケーションをプロダクション対応のAPIやアシスタントに変換できます。\\n\\nLangChainは、LLMや関連技術（埋め込みモデルやベクトルストアなど）の標準インターフェースを実装し、数百のプロバイダーと統合しています。これにより、開発者は多様なツールやサービスと連携しやすくなっています。'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Any\n",
    "\n",
    "from langchain_cohere import CohereRerank\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def rerank(inp: dict[str, Any], top_n: int = 3) -> list[Document]:\n",
    "    question = inp[\"question\"]\n",
    "    documents = inp[\"documents\"]\n",
    "\n",
    "    cohere_reranker = CohereRerank(model=\"rerank-multilingual-v3.0\", top_n=top_n)\n",
    "\n",
    "    return cohere_reranker.compress_documents(documents=documents, query=question)\n",
    "\n",
    "rerank_rag_chain = (\n",
    "    {\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"documents\": retriever,\n",
    "    }\n",
    "    | RunnablePassthrough.assign(context=rerank)\n",
    "    |prompt | model | StrOutputParser()\n",
    ")\n",
    "\n",
    "rerank_rag_chain.invoke(\"LangChainの概要を教えて\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20001327",
   "metadata": {},
   "source": [
    "## 6.4 Attempt to use several `retriever`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff884bff",
   "metadata": {},
   "source": [
    "Sometimes we want to retrieve different type of documents like LangChain's official document or WEB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f88957",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import TavilySearchAPIRetriever\n",
    "\n",
    "# Instantiate a `langchain document retriever`\n",
    "langchain_document_retriever = retriever.with_config(\n",
    "    {\"run_name\": \"langchain_document_retriever\"}\n",
    ")\n",
    "\n",
    "# Instantiate a `web api retriever` and allow it to retrieve 3 relevant documents\n",
    "web_retriever = TavilySearchAPIRetriever(k=3).with_config(\n",
    "    {\"run_name\": \"web_retriever\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd02e8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Route(str, Enum):\n",
    "    langchain_document = \"langchain_document\"\n",
    "    web = \"web\"\n",
    "\n",
    "class RouteOutput(BaseModel):\n",
    "    '''\n",
    "    This `class` is used to fix the format of model's output\n",
    "    '''\n",
    "    route: Route\n",
    "\n",
    "route_prompt = ChatPromptTemplate.from_template(\"\"\"\\\n",
    "質問に回答するために適切なRetrieverを選択してください。\n",
    "質問：{question}\n",
    "\"\"\")\n",
    "\n",
    "route_chain = (\n",
    "    route_prompt\n",
    "    | model.with_structured_output(RouteOutput)\n",
    "    | (lambda x: x.route)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf801e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChainは、大規模言語モデル（LLMs）を活用したアプリケーションを開発するためのフレームワークです。このフレームワークは、LLMアプリケーションのライフサイクル全体を簡素化することを目的としています。具体的には、以下の3つのステージでサポートを提供します：\\n\\n1. **開発**: LangChainのオープンソースコンポーネントやサードパーティの統合を利用してアプリケーションを構築できます。また、LangGraphを使用して、ストリーミングや人間の介入をサポートする状態保持エージェントを構築することができます。\\n\\n2. **プロダクション化**: LangSmithを使用して、アプリケーションを検査、監視、評価し、継続的に最適化して自信を持ってデプロイできます。\\n\\n3. **デプロイメント**: LangGraph Platformを使用して、LangGraphアプリケーションをプロダクション対応のAPIやアシスタントに変換できます。\\n\\nLangChainは、LLMや関連技術（埋め込みモデルやベクトルストアなど）の標準インターフェースを実装し、数百のプロバイダーと統合しています。これにより、開発者は迅速にアプリケーションを構築し、デプロイすることが可能です。'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def routed_retriever(inp: dict[str, Any]) -> list[Document]:\n",
    "    question = inp[\"question\"]\n",
    "    route = inp[\"route\"]\n",
    "\n",
    "    if route == Route.langchain_document:\n",
    "        return langchain_document_retriever.invoke(question)\n",
    "    elif route == Route.web:\n",
    "        return web_retriever.invoke(question)\n",
    "    \n",
    "    raise ValueError(f\"Unkonw route: {route}\")\n",
    "\n",
    "route_rag_chain = (\n",
    "    {\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"route\": route_chain,\n",
    "    }\n",
    "    # `RunnablePassthrough.assign()` is used to pass `question` and `route` arguments to `routed_retriever` function.\n",
    "    | RunnablePassthrough.assign(context=routed_retriever)\n",
    "    | prompt | model | StrOutputParser()\n",
    ")\n",
    "\n",
    "route_rag_chain.invoke(\"LangChainの概要を教えて\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca0c01a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2025年7月29日の東京の天気は「曇時々晴」で、気温は最高32℃、最低25℃です。'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "route_rag_chain.invoke(\"東京の今日(2025年7月29日)の天気は?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edccf49c",
   "metadata": {},
   "source": [
    "## 6.5 Hybrid Retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1f0b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import BM25Retriever\n",
    "\n",
    "chroma_retriever = retriever.with_config(\n",
    "    {\"run_name\": \"chroma_retriever\"}\n",
    ")\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_documents(documents).with_config(\n",
    "    {\"run_name\": \"bm25_retriever\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9429ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "hybrid_retriever = (\n",
    "    RunnableParallel({\n",
    "        \"chroma_documents\": chroma_retriever,\n",
    "        \"bm25_documents\": bm25_retriever,\n",
    "    })\n",
    "    | (lambda x: [x[\"chroma_documents\"], x[\"bm25_documents\"]])\n",
    "    | reciprocal_rank_fusion\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f281f0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChainは、大規模言語モデル（LLMs）を活用したアプリケーションを開発するためのフレームワークです。このフレームワークは、LLMアプリケーションのライフサイクル全体を簡素化することを目的としています。具体的には、以下の3つのステージでサポートを提供します：\\n\\n1. **開発**: LangChainのオープンソースコンポーネントやサードパーティの統合を利用してアプリケーションを構築できます。また、LangGraphを使用して、ストリーミングや人間の介入をサポートするステートフルなエージェントを構築することができます。\\n\\n2. **プロダクション化**: LangSmithを使用してアプリケーションを検査、監視、評価し、継続的に最適化して自信を持ってデプロイできます。\\n\\n3. **デプロイメント**: LangGraph Platformを使用して、LangGraphアプリケーションをプロダクション対応のAPIやアシスタントに変換できます。\\n\\nLangChainは、LLMや関連技術（埋め込みモデルやベクトルストアなど）の標準インターフェースを実装し、数百のプロバイダーと統合しています。これにより、開発者は多様なツールやサービスを活用して、より強力で柔軟なアプリケーションを構築することができます。'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hybrid_rag_chain = (\n",
    "    {\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"context\": hybrid_retriever,\n",
    "    }\n",
    "    | prompt | model | StrOutputParser()\n",
    ")\n",
    "\n",
    "hybrid_rag_chain.invoke(\"LangChainの概要を教えて\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bccbf2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent-py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
